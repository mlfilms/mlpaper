
@inproceedings{jeong_image_2018,
	address = {Shanghai},
	title = {Image {Preprocessing} for {Efficient} {Training} of {YOLO} {Deep} {Learning} {Networks}},
	isbn = {978-1-5386-3649-7},
	url = {https://ieeexplore.ieee.org/document/8367193/},
	doi = {10.1109/BigComp.2018.00113},
	abstract = {Every artificial intelligence system needs big data for training. In particular, artificial intelligence for object detection requires a lot of images for training. It is possible to obtain training images from internet by web crawler. In most cases, however, images collected by the crawler are often not refined. In other words, it can’t be used as training data in any artificial intelligence platform without proper pre-processing. This paper describes a pre-processing system for images collected by a crawler for YOLO training. And it will proved that this system is capable of efficient training.},
	language = {en},
	urldate = {2019-06-14},
	booktitle = {2018 {IEEE} {International} {Conference} on {Big} {Data} and {Smart} {Computing} ({BigComp})},
	publisher = {IEEE},
	author = {Jeong, Hyeok-June and Park, Kyeong-Sik and Ha, Young-Guk},
	month = jan,
	year = {2018},
	pages = {635--637},
	file = {Jeong et al. - 2018 - Image Preprocessing for Efficient Training of YOLO.pdf:C\:\\Users\\Eric Minor\\Zotero\\storage\\8UU562HJ\\Jeong et al. - 2018 - Image Preprocessing for Efficient Training of YOLO.pdf:application/pdf}
}

@article{zhang_deep_2017,
	title = {Deep {Reinforcement} {Learning} for {Visual} {Object} {Tracking} in {Videos}},
	url = {http://arxiv.org/abs/1701.08936},
	abstract = {In this paper we introduce a fully end-to-end approach for visual tracking in videos that learns to predict the bounding box locations of a target object at every frame. An important insight is that the tracking problem can be considered as a sequential decision-making process and historical semantics encode highly relevant information for future decisions. Based on this intuition, we formulate our model as a recurrent convolutional neural network agent that interacts with a video overtime, and our model can be trained with reinforcement learning (RL) algorithms to learn good tracking policies that pay attention to continuous, inter-frame correlation and maximize tracking performance in the long run. The proposed tracking algorithm achieves state-of-the-art performance in an existing tracking benchmark and operates at frame-rates faster than realtime. To the best of our knowledge, our tracker is the ﬁrst neural-network tracker that combines convolutional and recurrent networks with RL algorithms.},
	language = {en},
	urldate = {2019-06-14},
	journal = {arXiv:1701.08936 [cs]},
	author = {Zhang, Da and Maei, Hamid and Wang, Xin and Wang, Yuan-Fang},
	month = jan,
	year = {2017},
	note = {arXiv: 1701.08936},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
	file = {Zhang et al. - 2017 - Deep Reinforcement Learning for Visual Object Trac.pdf:C\:\\Users\\Eric Minor\\Zotero\\storage\\U493MEF4\\Zhang et al. - 2017 - Deep Reinforcement Learning for Visual Object Trac.pdf:application/pdf}
}

@article{walters_machine_2019,
	title = {Machine learning topological defects of confined liquid crystals in two dimensions},
	volume = {99},
	issn = {2470-0045, 2470-0053},
	url = {https://link.aps.org/doi/10.1103/PhysRevE.99.062701},
	doi = {10.1103/PhysRevE.99.062701},
	language = {en},
	number = {6},
	urldate = {2019-06-14},
	journal = {Physical Review E},
	author = {Walters, Michael and Wei, Qianshi and Chen, Jeff Z. Y.},
	month = jun,
	year = {2019},
	file = {Walters et al. - 2019 - Machine learning topological defects of confined l.pdf:C\:\\Users\\Eric Minor\\Zotero\\storage\\3Y2TYR77\\Walters et al. - 2019 - Machine learning topological defects of confined l.pdf:application/pdf}
}

@article{chng_machine_2017,
	title = {Machine {Learning} {Phases} of {Strongly} {Correlated} {Fermions}},
	volume = {7},
	issn = {2160-3308},
	url = {https://link.aps.org/doi/10.1103/PhysRevX.7.031038},
	doi = {10.1103/PhysRevX.7.031038},
	language = {en},
	number = {3},
	urldate = {2019-06-14},
	journal = {Physical Review X},
	author = {Ch’ng, Kelvin and Carrasquilla, Juan and Melko, Roger G. and Khatami, Ehsan},
	month = aug,
	year = {2017},
	file = {Ch’ng et al. - 2017 - Machine Learning Phases of Strongly Correlated Fer.pdf:C\:\\Users\\Eric Minor\\Zotero\\storage\\DRQ8BXZ8\\Ch’ng et al. - 2017 - Machine Learning Phases of Strongly Correlated Fer.pdf:application/pdf}
}

@article{wang_physics-informed_2017,
	title = {Physics-informed machine learning approach for reconstructing {Reynolds} stress modeling discrepancies based on {DNS} data},
	volume = {2},
	issn = {2469-990X},
	url = {https://link.aps.org/doi/10.1103/PhysRevFluids.2.034603},
	doi = {10.1103/PhysRevFluids.2.034603},
	language = {en},
	number = {3},
	urldate = {2019-06-14},
	journal = {Physical Review Fluids},
	author = {Wang, Jian-Xun and Wu, Jin-Long and Xiao, Heng},
	month = mar,
	year = {2017},
	file = {Wang et al. - 2017 - Physics-informed machine learning approach for rec.pdf:C\:\\Users\\Eric Minor\\Zotero\\storage\\FKPX9KNY\\Wang et al. - 2017 - Physics-informed machine learning approach for rec.pdf:application/pdf}
}

@article{hu_discovering_2017,
	title = {Discovering phases, phase transitions, and crossovers through unsupervised machine learning: {A} critical examination},
	volume = {95},
	issn = {2470-0045, 2470-0053},
	shorttitle = {Discovering phases, phase transitions, and crossovers through unsupervised machine learning},
	url = {https://link.aps.org/doi/10.1103/PhysRevE.95.062122},
	doi = {10.1103/PhysRevE.95.062122},
	language = {en},
	number = {6},
	urldate = {2019-06-14},
	journal = {Physical Review E},
	author = {Hu, Wenjian and Singh, Rajiv R. P. and Scalettar, Richard T.},
	month = jun,
	year = {2017},
	file = {Hu et al. - 2017 - Discovering phases, phase transitions, and crossov.pdf:C\:\\Users\\Eric Minor\\Zotero\\storage\\RJ3Q2W9J\\Hu et al. - 2017 - Discovering phases, phase transitions, and crossov.pdf:application/pdf}
}

@article{beach_machine_2018,
	title = {Machine learning vortices at the {Kosterlitz}-{Thouless} transition},
	volume = {97},
	issn = {2469-9950, 2469-9969},
	url = {https://link.aps.org/doi/10.1103/PhysRevB.97.045207},
	doi = {10.1103/PhysRevB.97.045207},
	language = {en},
	number = {4},
	urldate = {2019-06-14},
	journal = {Physical Review B},
	author = {Beach, Matthew J. S. and Golubeva, Anna and Melko, Roger G.},
	month = jan,
	year = {2018},
	file = {Beach et al. - 2018 - Machine learning vortices at the Kosterlitz-Thoule.pdf:C\:\\Users\\Eric Minor\\Zotero\\storage\\EQMRZTMK\\Beach et al. - 2018 - Machine learning vortices at the Kosterlitz-Thoule.pdf:application/pdf}
}

@article{deng_machine_2017,
	title = {Machine learning topological states},
	volume = {96},
	issn = {2469-9950, 2469-9969},
	url = {https://link.aps.org/doi/10.1103/PhysRevB.96.195145},
	doi = {10.1103/PhysRevB.96.195145},
	language = {en},
	number = {19},
	urldate = {2019-06-14},
	journal = {Physical Review B},
	author = {Deng, Dong-Ling and Li, Xiaopeng and Das Sarma, S.},
	month = nov,
	year = {2017},
	file = {Deng et al. - 2017 - Machine learning topological states.pdf:C\:\\Users\\Eric Minor\\Zotero\\storage\\DP9IYA7Q\\Deng et al. - 2017 - Machine learning topological states.pdf:application/pdf}
}

@article{wang_machine_2017,
	title = {Machine learning of frustrated classical spin models. {I}. {Principal} component analysis},
	volume = {96},
	issn = {2469-9950, 2469-9969},
	url = {https://link.aps.org/doi/10.1103/PhysRevB.96.144432},
	doi = {10.1103/PhysRevB.96.144432},
	language = {en},
	number = {14},
	urldate = {2019-06-14},
	journal = {Physical Review B},
	author = {Wang, Ce and Zhai, Hui},
	month = oct,
	year = {2017},
	file = {Wang and Zhai - 2017 - Machine learning of frustrated classical spin mode.pdf:C\:\\Users\\Eric Minor\\Zotero\\storage\\5IEWVNLM\\Wang and Zhai - 2017 - Machine learning of frustrated classical spin mode.pdf:application/pdf}
}

@article{wang_discovering_2016,
	title = {Discovering phase transitions with unsupervised learning},
	volume = {94},
	issn = {2469-9950, 2469-9969},
	url = {https://link.aps.org/doi/10.1103/PhysRevB.94.195105},
	doi = {10.1103/PhysRevB.94.195105},
	language = {en},
	number = {19},
	urldate = {2019-06-14},
	journal = {Physical Review B},
	author = {Wang, Lei},
	month = nov,
	year = {2016},
	file = {Wang - 2016 - Discovering phase transitions with unsupervised le.pdf:C\:\\Users\\Eric Minor\\Zotero\\storage\\56RCZIX3\\Wang - 2016 - Discovering phase transitions with unsupervised le.pdf:application/pdf}
}

@article{carrasquilla_machine_2017,
	title = {Machine learning phases of matter},
	volume = {13},
	issn = {1745-2473, 1745-2481},
	url = {http://www.nature.com/articles/nphys4035},
	doi = {10.1038/nphys4035},
	language = {en},
	number = {5},
	urldate = {2019-06-14},
	journal = {Nature Physics},
	author = {Carrasquilla, Juan and Melko, Roger G.},
	month = may,
	year = {2017},
	pages = {431--434},
	file = {Carrasquilla and Melko - 2017 - Machine learning phases of matter.pdf:C\:\\Users\\Eric Minor\\Zotero\\storage\\BL3P9CK8\\Carrasquilla and Melko - 2017 - Machine learning phases of matter.pdf:application/pdf}
}

@article{redmon_yolo9000:_2016,
	title = {{YOLO}9000: {Better}, {Faster}, {Stronger}},
	shorttitle = {{YOLO}9000},
	url = {http://arxiv.org/abs/1612.08242},
	abstract = {We introduce YOLO9000, a state-of-the-art, real-time object detection system that can detect over 9000 object categories. First we propose various improvements to the YOLO detection method, both novel and drawn from prior work. The improved model, YOLOv2, is state-of-the-art on standard detection tasks like PASCAL VOC and COCO. Using a novel, multi-scale training method the same YOLOv2 model can run at varying sizes, offering an easy tradeoff between speed and accuracy. At 67 FPS, YOLOv2 gets 76.8 mAP on VOC 2007. At 40 FPS, YOLOv2 gets 78.6 mAP, outperforming state-of-the-art methods like Faster RCNN with ResNet and SSD while still running signiﬁcantly faster. Finally we propose a method to jointly train on object detection and classiﬁcation. Using this method we train YOLO9000 simultaneously on the COCO detection dataset and the ImageNet classiﬁcation dataset. Our joint training allows YOLO9000 to predict detections for object classes that don’t have labelled detection data. We validate our approach on the ImageNet detection task. YOLO9000 gets 19.7 mAP on the ImageNet detection validation set despite only having detection data for 44 of the 200 classes. On the 156 classes not in COCO, YOLO9000 gets 16.0 mAP. But YOLO can detect more than just 200 classes; it predicts detections for more than 9000 different object categories. And it still runs in real-time.},
	language = {en},
	urldate = {2019-06-20},
	journal = {arXiv:1612.08242 [cs]},
	author = {Redmon, Joseph and Farhadi, Ali},
	month = dec,
	year = {2016},
	note = {arXiv: 1612.08242},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Redmon and Farhadi - 2016 - YOLO9000 Better, Faster, Stronger.pdf:C\:\\Users\\Eric Minor\\Zotero\\storage\\VW7KD3XE\\Redmon and Farhadi - 2016 - YOLO9000 Better, Faster, Stronger.pdf:application/pdf}
}

@article{redmon_you_2015,
	title = {You {Only} {Look} {Once}: {Unified}, {Real}-{Time} {Object} {Detection}},
	shorttitle = {You {Only} {Look} {Once}},
	url = {http://arxiv.org/abs/1506.02640},
	abstract = {We present YOLO, a new approach to object detection. Prior work on object detection repurposes classiﬁers to perform detection. Instead, we frame object detection as a regression problem to spatially separated bounding boxes and associated class probabilities. A single neural network predicts bounding boxes and class probabilities directly from full images in one evaluation. Since the whole detection pipeline is a single network, it can be optimized end-to-end directly on detection performance.},
	language = {en},
	urldate = {2019-06-20},
	journal = {arXiv:1506.02640 [cs]},
	author = {Redmon, Joseph and Divvala, Santosh and Girshick, Ross and Farhadi, Ali},
	month = jun,
	year = {2015},
	note = {arXiv: 1506.02640},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Redmon et al. - 2015 - You Only Look Once Unified, Real-Time Object Dete.pdf:C\:\\Users\\Eric Minor\\Zotero\\storage\\CZNW4B2C\\Redmon et al. - 2015 - You Only Look Once Unified, Real-Time Object Dete.pdf:application/pdf}
}

@article{girshick_rich_2013,
	title = {Rich feature hierarchies for accurate object detection and semantic segmentation},
	url = {http://arxiv.org/abs/1311.2524},
	abstract = {Object detection performance, as measured on the canonical PASCAL VOC dataset, has plateaued in the last few years. The best-performing methods are complex ensemble systems that typically combine multiple low-level image features with high-level context. In this paper, we propose a simple and scalable detection algorithm that improves mean average precision (mAP) by more than 30\% relative to the previous best result on VOC 2012—achieving a mAP of 53.3\%. Our approach combines two key insights: (1) one can apply high-capacity convolutional neural networks (CNNs) to bottom-up region proposals in order to localize and segment objects and (2) when labeled training data is scarce, supervised pre-training for an auxiliary task, followed by domain-speciﬁc ﬁne-tuning, yields a signiﬁcant performance boost. Since we combine region proposals with CNNs, we call our method R-CNN: Regions with CNN features. We also compare R-CNN to OverFeat, a recently proposed sliding-window detector based on a similar CNN architecture. We ﬁnd that R-CNN outperforms OverFeat by a large margin on the 200-class ILSVRC2013 detection dataset. Source code for the complete system is available at http://www.cs.berkeley.edu/˜rbg/rcnn.},
	language = {en},
	urldate = {2019-06-20},
	journal = {arXiv:1311.2524 [cs]},
	author = {Girshick, Ross and Donahue, Jeff and Darrell, Trevor and Malik, Jitendra},
	month = nov,
	year = {2013},
	note = {arXiv: 1311.2524},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: Extended version of our CVPR 2014 paper; latest update (v5) includes results using deeper networks (see Appendix G. Changelog)},
	file = {Girshick et al. - 2013 - Rich feature hierarchies for accurate object detec.pdf:C\:\\Users\\Eric Minor\\Zotero\\storage\\RDGYKTU9\\Girshick et al. - 2013 - Rich feature hierarchies for accurate object detec.pdf:application/pdf}
}

@article{ren_faster_2017,
	title = {Faster {R}-{CNN}: {Towards} {Real}-{Time} {Object} {Detection} with {Region} {Proposal} {Networks}},
	volume = {39},
	issn = {0162-8828, 2160-9292},
	shorttitle = {Faster {R}-{CNN}},
	url = {http://ieeexplore.ieee.org/document/7485869/},
	doi = {10.1109/TPAMI.2016.2577031},
	abstract = {State-of-the-art object detection networks depend on region proposal algorithms to hypothesize object locations. Advances like SPPnet [7] and Fast R-CNN [5] have reduced the running time of these detection networks, exposing region proposal computation as a bottleneck. In this work, we introduce a Region Proposal Network (RPN) that shares full-image convolutional features with the detection network, thus enabling nearly cost-free region proposals. An RPN is a fully-convolutional network that simultaneously predicts object bounds and objectness scores at each position. RPNs are trained end-to-end to generate highquality region proposals, which are used by Fast R-CNN for detection. With a simple alternating optimization, RPN and Fast R-CNN can be trained to share convolutional features. For the very deep VGG-16 model [19], our detection system has a frame rate of 5fps (including all steps) on a GPU, while achieving state-of-the-art object detection accuracy on PASCAL VOC 2007 (73.2\% mAP) and 2012 (70.4\% mAP) using 300 proposals per image. Code is available at https://github.com/ShaoqingRen/faster\_rcnn.},
	language = {en},
	number = {6},
	urldate = {2019-06-20},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Ren, Shaoqing and He, Kaiming and Girshick, Ross and Sun, Jian},
	month = jun,
	year = {2017},
	pages = {1137--1149},
	file = {Ren et al. - 2017 - Faster R-CNN Towards Real-Time Object Detection w.pdf:C\:\\Users\\Eric Minor\\Zotero\\storage\\NX6D2G8A\\Ren et al. - 2017 - Faster R-CNN Towards Real-Time Object Detection w.pdf:application/pdf}
}

@article{nehring_schlieren_1972,
	title = {On the schlieren texture in nematic and smectic liquid crystals},
	volume = {68},
	issn = {0300-9238},
	url = {http://xlink.rsc.org/?DOI=f29726800001},
	doi = {10.1039/f29726800001},
	language = {en},
	urldate = {2019-06-28},
	journal = {Journal of the Chemical Society, Faraday Transactions 2},
	author = {Nehring, J�rgen and Saupe, Alfred},
	year = {1972},
	pages = {1},
	file = {Nehring and Saupe - 1972 - On the schlieren texture in nematic and smectic li.pdf:C\:\\Users\\Eric Minor\\Zotero\\storage\\4A7MEY59\\Nehring and Saupe - 1972 - On the schlieren texture in nematic and smectic li.pdf:application/pdf}
}

@article{girshick_fast_2015,
	title = {Fast {R}-{CNN}},
	url = {http://arxiv.org/abs/1504.08083},
	abstract = {This paper proposes a Fast Region-based Convolutional Network method (Fast R-CNN) for object detection. Fast R-CNN builds on previous work to efﬁciently classify object proposals using deep convolutional networks. Compared to previous work, Fast R-CNN employs several innovations to improve training and testing speed while also increasing detection accuracy. Fast R-CNN trains the very deep VGG16 network 9× faster than R-CNN, is 213× faster at test-time, and achieves a higher mAP on PASCAL VOC 2012. Compared to SPPnet, Fast R-CNN trains VGG16 3× faster, tests 10× faster, and is more accurate. Fast R-CNN is implemented in Python and C++ (using Caffe) and is available under the open-source MIT License at https: //github.com/rbgirshick/fast-rcnn.},
	language = {en},
	urldate = {2019-07-16},
	journal = {arXiv:1504.08083 [cs]},
	author = {Girshick, Ross},
	month = apr,
	year = {2015},
	note = {arXiv: 1504.08083},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: To appear in ICCV 2015},
	file = {Girshick - 2015 - Fast R-CNN.pdf:C\:\\Users\\Eric Minor\\Zotero\\storage\\3VL5PKVH\\Girshick - 2015 - Fast R-CNN.pdf:application/pdf}
}

@article{aksoy_feature_2001,
	title = {Feature normalization and likelihood-based similarity measures for image retrieval},
	volume = {22},
	issn = {01678655},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0167865500001124},
	doi = {10.1016/S0167-8655(00)00112-4},
	abstract = {Distance measures like the Euclidean distance are used to measure similarity between images in content-based image retrieval. Such geometric measures implicitly assign more weighting to features with large ranges than those with small ranges. This paper discusses the eﬀects of ﬁve feature normalization methods on retrieval performance. We also describe two likelihood ratio-based similarity measures that perform signiﬁcantly better than the commonly used geometric approaches like the Lp metrics.},
	language = {en},
	number = {5},
	urldate = {2019-07-16},
	journal = {Pattern Recognition Letters},
	author = {Aksoy, Selim and Haralick, Robert M.},
	month = apr,
	year = {2001},
	pages = {563--582},
	file = {Aksoy and Haralick - 2001 - Feature normalization and likelihood-based similar.pdf:C\:\\Users\\Eric Minor\\Zotero\\storage\\F69T8VWF\\Aksoy and Haralick - 2001 - Feature normalization and likelihood-based similar.pdf:application/pdf}
}

@article{lever_model_2016,
	title = {Model selection and overfitting},
	volume = {13},
	issn = {1548-7091, 1548-7105},
	url = {http://www.nature.com/articles/nmeth.3968},
	doi = {10.1038/nmeth.3968},
	language = {en},
	number = {9},
	urldate = {2019-07-16},
	journal = {Nature Methods},
	author = {Lever, Jake and Krzywinski, Martin and Altman, Naomi},
	month = sep,
	year = {2016},
	pages = {703--704},
	file = {Lever et al. - 2016 - Model selection and overfitting.pdf:C\:\\Users\\Eric Minor\\Zotero\\storage\\D555XE82\\Lever et al. - 2016 - Model selection and overfitting.pdf:application/pdf}
}

@article{bishop_training_1995,
	title = {Training with {Noise} is {Equivalent} to {Tikhonov} {Regularization}},
	volume = {7},
	issn = {0899-7667, 1530-888X},
	url = {http://www.mitpressjournals.org/doi/10.1162/neco.1995.7.1.108},
	doi = {10.1162/neco.1995.7.1.108},
	language = {en},
	number = {1},
	urldate = {2019-07-16},
	journal = {Neural Computation},
	author = {Bishop, Chris M.},
	month = jan,
	year = {1995},
	pages = {108--116},
	file = {Bishop - 1995 - Training with Noise is Equivalent to Tikhonov Regu.pdf:C\:\\Users\\Eric Minor\\Zotero\\storage\\BZBBABRK\\Bishop - 1995 - Training with Noise is Equivalent to Tikhonov Regu.pdf:application/pdf}
}

@article{goodfellow_explaining_2014,
	title = {Explaining and {Harnessing} {Adversarial} {Examples}},
	url = {http://arxiv.org/abs/1412.6572},
	abstract = {Several machine learning models, including neural networks, consistently misclassify adversarial examples—inputs formed by applying small but intentionally worst-case perturbations to examples from the dataset, such that the perturbed input results in the model outputting an incorrect answer with high conﬁdence. Early attempts at explaining this phenomenon focused on nonlinearity and overﬁtting. We argue instead that the primary cause of neural networks’ vulnerability to adversarial perturbation is their linear nature. This explanation is supported by new quantitative results while giving the ﬁrst explanation of the most intriguing fact about them: their generalization across architectures and training sets. Moreover, this view yields a simple and fast method of generating adversarial examples. Using this approach to provide examples for adversarial training, we reduce the test set error of a maxout network on the MNIST dataset.},
	language = {en},
	urldate = {2019-07-16},
	journal = {arXiv:1412.6572 [cs, stat]},
	author = {Goodfellow, Ian J. and Shlens, Jonathon and Szegedy, Christian},
	month = dec,
	year = {2014},
	note = {arXiv: 1412.6572},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Goodfellow et al. - 2014 - Explaining and Harnessing Adversarial Examples.pdf:C\:\\Users\\Eric Minor\\Zotero\\storage\\U8GENIVE\\Goodfellow et al. - 2014 - Explaining and Harnessing Adversarial Examples.pdf:application/pdf}
}

@article{kaur_periodic_nodate,
	title = {Periodic {Noise} {Removal} in {Strain} and {Natural} {Images} {Using} 2-{D} {Fast} {Fourier} {Transform}},
	volume = {3},
	abstract = {This paper presents a 2-D FFT removal algorithm for reducing the periodic noise in natural and strain images. For the periodic pattern of the artifacts, we apply the 2-D FFT on the strain and natural images to extract and remove the peaks which are corresponding to periodic noise in the frequency domain. Further the mean filter applied to get more effective results. The performance of the proposed method is tested on both natural and strain images. The results of proposed method is compared with the mean filter based periodic noise removal and found that the proposed method significantly improved for the noise removal.},
	language = {en},
	number = {3},
	author = {Kaur, Mandeep and Kumar, Dinesh and Walia, Ekta and Sandhu, Manjit},
	pages = {5},
	file = {Kaur et al. - Periodic Noise Removal in Strain and Natural Image.pdf:C\:\\Users\\Eric Minor\\Zotero\\storage\\APW2XNIK\\Kaur et al. - Periodic Noise Removal in Strain and Natural Image.pdf:application/pdf}
}

@article{everingham_pascal_2010,
	title = {The {Pascal} {Visual} {Object} {Classes} ({VOC}) {Challenge}},
	volume = {88},
	issn = {0920-5691, 1573-1405},
	url = {http://link.springer.com/10.1007/s11263-009-0275-4},
	doi = {10.1007/s11263-009-0275-4},
	abstract = {The PASCAL Visual Object Classes (VOC) challenge is a benchmark in visual object category recognition and detection, providing the vision and machine learning communities with a standard dataset of images and annotation, and standard evaluation procedures. Organised annually from 2005 to present, the challenge and its associated dataset has become accepted as the benchmark for object detection.},
	language = {en},
	number = {2},
	urldate = {2019-07-17},
	journal = {International Journal of Computer Vision},
	author = {Everingham, Mark and Van Gool, Luc and Williams, Christopher K. I. and Winn, John and Zisserman, Andrew},
	month = jun,
	year = {2010},
	pages = {303--338},
	file = {Everingham et al. - 2010 - The Pascal Visual Object Classes (VOC) Challenge.pdf:C\:\\Users\\Eric Minor\\Zotero\\storage\\SKPAQRZA\\Everingham et al. - 2010 - The Pascal Visual Object Classes (VOC) Challenge.pdf:application/pdf}
}

@inproceedings{chinchor_muc-4_1992,
	address = {Stroudsburg, PA, USA},
	series = {{MUC}4 '92},
	title = {{MUC}-4 {Evaluation} {Metrics}},
	isbn = {1-55860-273-9},
	url = {https://doi.org/10.3115/1072064.1072067},
	doi = {10.3115/1072064.1072067},
	booktitle = {Proceedings of the 4th {Conference} on {Message} {Understanding}},
	publisher = {Association for Computational Linguistics},
	author = {Chinchor, Nancy},
	year = {1992},
	note = {event-place: McLean, Virginia},
	pages = {22--29}
}

@article{koppel_importance_2006,
	title = {{THE} {IMPORTANCE} {OF} {NEUTRAL} {EXAMPLES} {FOR} {LEARNING} {SENTIMENT}},
	volume = {22},
	issn = {0824-7935, 1467-8640},
	url = {http://doi.wiley.com/10.1111/j.1467-8640.2006.00276.x},
	doi = {10.1111/j.1467-8640.2006.00276.x},
	abstract = {Most research on learning to identify sentiment ignores “neutral” examples, learning only from examples of signiﬁcant (positive or negative) polarity. We show that it is crucial to use neutral examples in learning polarity for a variety of reasons. Learning from negative and positive examples alone will not permit accurate classiﬁcation of neutral examples. Moreover, the use of neutral training examples in learning facilitates better distinction between positive and negative examples.},
	language = {en},
	number = {2},
	urldate = {2019-07-18},
	journal = {Computational Intelligence},
	author = {Koppel, Moshe and Schler, Jonathan},
	month = may,
	year = {2006},
	pages = {100--109},
	file = {Koppel and Schler - 2006 - THE IMPORTANCE OF NEUTRAL EXAMPLES FOR LEARNING SE.pdf:C\:\\Users\\Eric Minor\\Zotero\\storage\\NE2A34B7\\Koppel and Schler - 2006 - THE IMPORTANCE OF NEUTRAL EXAMPLES FOR LEARNING SE.pdf:application/pdf}
}

@inproceedings{lawrence_lessons_1997,
	title = {Lessons in neural network training: {Overfitting} may be harder than expected},
	booktitle = {{AAAI}/{IAAI}},
	publisher = {Citeseer},
	author = {Lawrence, Steve and Giles, C Lee and Tsoi, Ah Chung},
	year = {1997},
	pages = {540--545}
}

@phdthesis{harth_episodes_2016,
	address = {Magdeburg, Germany},
	title = {Episodes of the {Life} and {Death} of {Thin} {Fluid} {Membranes}},
	url = {http://dx.doi.org/10.25673/4391},
	school = {Otto-von-Guericke-Universität Magdeburg},
	author = {Harth, Kirsten},
	month = mar,
	year = {2016}
}